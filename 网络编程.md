1. TCP soctet交互流程？

    - 服务端：socket -> bind -> listen -> accept -> recv/send -> close。

    - 客户端：socket -> connect -> send/recv -> close。
        
    - 服务器：
        - 创建socket -> int socket(int domain, int type, int protocol);
            - domain：协议域，决定了socket的地址类型，IPv4为AF_INET。
      
            - type：指定socket类型，SOCK_STREAM为TCP连接。
      
            - protocol：指定协议。IPPROTO_TCP表示TCP协议，为0时自动选择type默认协议。
      
        - 绑定socket和端口号 -> int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen);
            - sockfd：socket返回的套接字描述符，类似于文件描述符fd。
      
            - addr：有个sockaddr类型数据的指针，指向的是被绑定结构变量。
            ```C++
                // IPv4的sockaddr地址结构
                struct sockaddr_in {
                    sa_family_t sin_family;    // 协议类型，AF_INET
                    in_port_t sin_port;    // 端口号
                    struct in_addr sin_addr;    // IP地址
                };
                struct in_addr {
                    uint32_t s_addr;
                }
            ```
      
            - addrlen：地址长度。
      
        - 监听端口号 -> int listen(int sockfd, int backlog);
            - sockfd：要监听的sock描述字。
      
            - backlog：socket可以排队的最大连接数。
      
        - 接收用户请求 -> int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen);
            - sockfd：服务器socket描述字。
      
            - addr：指向地址结构指针。
      
            - addrlen：协议地址长度。
      
            - 注：一旦accept某个客户机请求成功将返回一个全新的描述符用于标识具体客户的TCP连接。
      
        - 从socket中读取字符 -> ssize_t read(int fd, void *buf, size_t count);
            - fd：连接描述字。
      
            - buf：缓冲区buf。
      
            - count：缓冲区长度。
      
            - 注：大于0表示读取的字节数，返回0表示文件读取结束，小于0表示发生错误。
        
        - 关闭socket -> int close(int fd);
        
            - fd：accept返回的连接描述字，每个连接有一个，生命周期为连接周期。
        
            - 注：sockfd是监听描述字，一个服务器只有一个，用于监听是否有连接；fd是连接描述字，用于每个连接的操作。
    
    - 客户机：
        - 创建socket -> int socket(int domain, int type, int protocol);
    
        - 连接指定计算机 -> int connect(int sockfd, struct sockaddr* addr, socklen_t addrlen);
            - sockfd客户端的sock描述字。
    
            - addr：服务器的地址。
    
            - addrlen：socket地址长度。
    
        - 向socket写入信息 -> ssize_t write(int fd, const void *buf, size_t count);
            - fd、buf、count：同read中意义。
    
            - 大于0表示写了部分或全部数据，小于0表示出错。
    
        - 关闭oscket -> int close(int fd);
            - fd：同服务器端fd。
            
 ## I/O模型
 
 一个输入操作通常包括两个阶段：

- 等待数据准备好
- 从内核向进程复制数据

对于一个套接字上的输入操作，第一步通常涉及等待数据从网络中到达。当所等待数据到达时，它被复制到内核中的某个缓冲区。第二步就是把数据从内核缓冲区复制到应用进程缓冲区。

Unix 有五种 I/O 模型：

- 阻塞式 I/O
- 非阻塞式 I/O
- I/O 复用（select 和 poll）
- 信号驱动式 I/O（SIGIO）
- 异步 I/O（AIO）

1. 阻塞式I/O

如果内核缓冲没有数据可读时，read()系统调用会一直等待有数据到来后才从阻塞态中返回。

2. 非阻塞式I/O

非阻塞I/O在遇到上述情况时会立即返回给用户态进程一个返回值，并设置errno为EAGAIN。

3.I/O复用

使用 select 或者 poll 等待数据，并且可以等待多个套接字中的任何一个变为可读。这一过程会被阻塞，当某一个套接字可读时返回，之后再使用 recvfrom 把数据从内核复制到进程中。

它可以让单个进程具有处理多个 I/O 事件的能力。又被称为 Event Driven I/O，即事件驱动 I/O。

4. 信号驱动 I/O

应用进程使用 sigaction 系统调用，内核立即返回，应用进程可以继续执行，也就是说等待数据阶段应用进程是非阻塞的。内核在数据到达时向应用进程发送 SIGIO 信号，应用进程收到之后在信号处理程序中调用 recvfrom 将数据从内核复制到应用进程中。

相比于非阻塞式 I/O 的轮询方式，信号驱动 I/O 的 CPU 利用率更高。

5. 异步 I/O

应用进程执行 aio_read 系统调用会立即返回，应用进程可以继续执行，不会被阻塞，内核会在所有操作完成之后向应用进程发送信号。

异步 I/O 与信号驱动 I/O 的区别在于，异步 I/O 的信号是通知应用进程 I/O 完成，而信号驱动 I/O 的信号是通知应用进程可以开始 I/O。

 ## I/O复用
 
1. epoll和select及poll区别？

文件描述符数量限制：select文件描述符数量受到限制，最大为2048（FD_SETSIZE），可重编内核修改但治标不治本；poll没有最大文件描述符数量限制；epoll没有最大文件描述符数量限制。

检查机制：select和poll会以遍历方式（轮询机制）检查每一个文件描述符以确定是否有I/O就绪，每次执行时间会随着连接数量的增加而线性增长；epoll则每次返回后只对活跃的文件描述符队列进行操作（每个描述符都通过回调函数实现，只有活跃的描述符会调用回调函数并添加至队列中）。当大量连接是非活跃连接时epoll相对于select和poll优势比较大，若大多为活跃连接则效率未必高（设计队列维护及红黑树创建）

数据传递方式：select和poll需要将FD_SET在内核空间和用户空间来回拷贝；epoll则避免了不必要的数据拷贝。

2. epoll中ET和LT模式的区别与实现原理？

LT：当 epoll_wait() 检测到描述符事件到达时，将此事件通知进程，进程可以不立即处理该事件，下次调用 epoll_wait() 会再次通知进程。是默认的一种模式，并且同时支持 Blocking 和 No-Blocking。

ET：和 LT 模式不同的是，通知之后进程必须立即处理事件，下次再调用 epoll_wait() 时不会再得到事件到达的通知。很大程度上减少了 epoll 事件被重复触发的次数，因此效率要比 LT 模式高。只支持 No-Blocking。
